{"cells":[{"cell_type":"code","source":["import re\nimport datetime\n\nfrom pyspark.sql import Row\n\nmonth_map = {'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n    'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12}\n\ndef parse_apache_time(s):\n    \"\"\" Convert Apache time format into a Python datetime object\n    Args:\n        s (str): date and time in Apache time format\n    Returns:\n        datetime: datetime object (ignore timezone for now)\n    \"\"\"\n    return datetime.datetime(int(s[7:11]),\n                             month_map[s[3:6]],\n                             int(s[0:2]),\n                             int(s[12:14]),\n                             int(s[15:17]),\n                             int(s[18:20]))\n\n\ndef parseApacheLogLine(logline):\n    \"\"\" Parse a line in the Apache Common Log format\n    Args:\n        logline (str): a line of text in the Apache Common Log format\n    Returns:\n        tuple: either a dictionary containing the parts of the Apache Access Log and 1,\n               or the original invalid log line and 0\n    \"\"\"\n    match = re.search(APACHE_ACCESS_LOG_PATTERN, logline)\n    if match is None:\n        return (logline, 0)\n    size_field = match.group(9)\n    if size_field == '-':\n        size = long(0)\n    else:\n        size = long(match.group(9))\n    return (Row(\n        host          = match.group(1),\n        client_identd = match.group(2),\n        user_id       = match.group(3),\n        date_time     = parse_apache_time(match.group(4)),\n        method        = match.group(5),\n        endpoint      = match.group(6),\n        protocol      = match.group(7),\n        response_code = int(match.group(8)),\n        content_size  = size\n    ), 1)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# A regular expression pattern to extract fields from the log line\nAPACHE_ACCESS_LOG_PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (.+?)\\s?(\\S+)?\" (\\d{3}) (\\S+)'"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["def parseLogs():\n    \"\"\" Read and parse log file \"\"\"\n    parsed_logs = (sc\n                   .textFile(\"/FileStore/tables/access_log_Jul95\")\n                   .map(parseApacheLogLine)\n                   .cache())\n\n    access_logs = (parsed_logs\n                   .filter(lambda s: s[1] == 1)\n                   .map(lambda s: s[0])\n                   .cache())\n\n    failed_logs = (parsed_logs\n                   .filter(lambda s: s[1] == 0)\n                   .map(lambda s: s[0]))\n    failed_logs_count = failed_logs.count()\n    if failed_logs_count > 0:\n        print 'Number of invalid logline: %d' % failed_logs.count()\n        for line in failed_logs.take(20):\n            print 'Invalid logline: %s' % line\n\n    print 'Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (parsed_logs.count(), access_logs.count(), failed_logs.count())\n    return parsed_logs, access_logs, failed_logs\n\n\nparsed_logs, access_logs, failed_logs = parseLogs()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Calculate statistics based on the content size.\ncontent_sizes = access_logs.map(lambda log: log.content_size).cache()\navg = content_sizes.reduce(lambda a, b : a + b) / content_sizes.count()\nmin = content_sizes.min()\nmax = content_sizes.max()\nprint 'Content Size, Avg: ',avg,' Min: ',min,' Max: ',max"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Response Code to Count\nresponseCodeToCount = (access_logs\n                       .map(lambda log: (log.response_code, 1))\n                       .reduceByKey(lambda a, b : a + b)\n                       .cache())\nresponseCodeToCountList = responseCodeToCount.take(100)\nprint 'Found %d response codes' % len(responseCodeToCountList)\nprint 'Response Code Counts: %s' % responseCodeToCountList"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Any hosts that has accessed the server more than 10 times.\nhostCountPairTuple = access_logs.map(lambda log: (log.host, 1))\n\nhostSum = hostCountPairTuple.reduceByKey(lambda a, b : a + b)\n\nhostMoreThan10 = hostSum.filter(lambda s: s[1] > 10)\n\nhostsPick20 = (hostMoreThan10\n               .map(lambda s: s[0])\n               .take(20))\n\nprint 'Any 20 hosts that have accessed more then 10 times: %s' % hostsPick20"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Top Endpoints\nendpointCounts = (access_logs\n                  .map(lambda log: (log.endpoint, 1))\n                  .reduceByKey(lambda a, b : a + b))\n\ntopEndpoints = endpointCounts.takeOrdered(10, lambda s: -1 * s[1])\n\nprint 'Top Ten Endpoints: %s' % topEndpoints"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# What are the top ten endpoints which did not have return code 200?\nnot200 = access_logs.filter(lambda log: log.response_code!=200)\n\nendpointCountPairTuple = not200.map(lambda log: (log.endpoint,1))\n\nendpointSum = endpointCountPairTuple.reduceByKey(lambda a,b: a+b)\n\ntopTenErrURLs = endpointSum.takeOrdered(10, lambda s: -1 * s[1])\nprint 'Top Ten failed URLs: %s' % topTenErrURLs"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#How many unique hosts are there in the entire log?\nhosts = access_logs.map(lambda log: log.host)\n\nuniqueHosts = hosts.distinct()\nuniqueHostCount = uniqueHosts.count()\nprint 'Unique hosts: %d' % uniqueHostCount"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#the number of unique hosts in the entire log on a day-by-day basis.\ndayToHostPairTuple = access_logs.map(lambda log:(log.date_time.day, log.host))\n\ndayGroupedHosts = dayToHostPairTuple.groupByKey()\ndayHostCount = dayGroupedHosts.map(lambda (day, hosts): (day, len(set(hosts)) ) )\n\ndailyHosts = dayHostCount.sortByKey().cache()\ndailyHostsList = dailyHosts.take(30)\nprint 'Unique hosts per day: %s' % dailyHostsList"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# the average number of requests on a day-by-day basis\ndayAndHostTuple = access_logs.map(lambda log:(log.date_time.day,1))\n\ngroupedByDay =  dayAndHostTuple.reduceByKey(lambda a, b: a + b)\n\nsortedByDay = groupedByDay.sortByKey()\n\navgDailyReqPerHost = (sortedByDay\n                      .join(dailyHosts)\n                      .map(lambda (x,y): (x , y[0] / y[1]))\n                      .sortByKey()\n                      .cache()\n                     )\n                      \navgDailyReqPerHostList = avgDailyReqPerHost.take(30)\nprint 'Average number of daily requests per Hosts is %s' % avgDailyReqPerHostList"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Counting 404 Response Codes\nbadRecords = (access_logs\n              .filter(lambda log: log.response_code==404)\n              .cache())\nprint 'Found %d 404 URLs' % badRecords.count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#Listing 404 Response Code Records\nbadEndpoints = badRecords.map(lambda log: log.endpoint)\n\nbadUniqueEndpoints = badEndpoints.distinct()\n\nbadUniqueEndpointsPick40 = badUniqueEndpoints.take(40)\nprint '404 URLS: %s' % badUniqueEndpointsPick40\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#Listing the Top Twenty 404 Response Code Endpoints\nbadEndpointsCountPairTuple = badRecords.map(lambda log: (log.endpoint,1))\n\nbadEndpointsSum = badEndpointsCountPairTuple.reduceByKey(lambda a,b:a+b)\n\nbadEndpointsTop20 = badEndpointsSum.takeOrdered(20,lambda s: -s[1])\nprint 'Top Twenty 404 URLs: %s' % badEndpointsTop20"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#Listing the Top Twenty-five 404 Response Code Hosts\nerrHostsCountPairTuple = badRecords.map(lambda log: (log.host,1))\n\nerrHostsSum = errHostsCountPairTuple.reduceByKey(lambda a,b:a+b)\n\nerrHostsTop25 = errHostsSum.takeOrdered(25,lambda s: -s[1])\nprint 'Top 25 hosts that generated errors: %s' % errHostsTop25"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#Listing 404 Response Codes per Day\nerrDateCountPairTuple = badRecords.map(lambda log:(log.date_time.day,1))\n\nerrDateSum = errDateCountPairTuple.reduceByKey(lambda a,b: a+b)\n\nerrDateSorted = (errDateSum\n                 .sortByKey()\n                 .cache()\n                )\n\nerrByDate = errDateSorted.take(30)\nprint '404 Errors by day: %s' % errByDate\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#Top Five Days for 404 Response Codes\ntopErrDate = errDateSorted.takeOrdered(5,lambda s:-s[1])\nprint 'Top Five dates for 404 requests: %s' % topErrDate"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#Hourly 404 Response Codes\nhourCountPairTuple = badRecords.map(lambda log:(log.date_time.hour,1))\n\nhourRecordsSum = hourCountPairTuple.reduceByKey(lambda a,b:a+b)\n\nhourRecordsSorted = (hourRecordsSum\n                     .sortByKey()\n                     .cache()\n                    )\n\nerrHourList = hourRecordsSorted.take(24)\nprint 'Top hours for 404 requests: %s' % errHourList"],"metadata":{},"outputs":[],"execution_count":18}],"metadata":{"name":"server_log_analysis","notebookId":3099991330498948},"nbformat":4,"nbformat_minor":0}
